{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain langchain-experimental openai tiktoken\n",
    "!pip install langchain langchain-experimental tiktoken langchain-openai tabulate langchain-community fastapi uvicorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## defining model#############\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"mysql://root:Qwert1234%@localhost:3306/limechat2\")\n",
    "db.table_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"C:/Users/suraj/OneDrive/Desktop/Personal/bhole/gen-AI/limechat/limebot_workbook.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for the prompt\n",
    "template = \"\"\"You are a helpful assistant.\n",
    "{chat_history}\n",
    "Human: {human_input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"chat_history\", \"human_input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Set up the LLM chain with memory and prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the main theme of the document?\"\n",
    "response = llm_chain.run({\"human_input\":\"Hi there!\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_chain.run({\"human_input\":\"send me songs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# des = pd.read_excel('files\\description.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# des.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# TEMPLATE = \"\"\"You are a SQLite expert. Given an input question and an sqlite relational database, create a syntactically correct SQLite query.For now, we are skipping the run of the sql query we are generating, but later we will query for at most {top_k} results using the LIMIT clause as per SQLite. \n",
    "\n",
    "# In description table, 'tags' column tells you about the data type of any entry in any of the columns(second column of description table) in any of the tables(first column of description table).\n",
    "# It also tells us whether any of the columns are primary key or not. Rest of the columns in description table are self explainatory.\n",
    "\n",
    "# Use the following format:\n",
    "# Question: \"Question here\"\n",
    "# SQLQuery: \"SQL Query to run\"\n",
    "\n",
    "# Only use the following tables:\n",
    "\n",
    "# {table_info}.\n",
    "\n",
    "# Question: {input}\"\"\"\n",
    "\n",
    "# PROMPT = PromptTemplate(\n",
    "#     input_variables=[\"input\", \"table_info\",\"top_k\"], template=TEMPLATE\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Also change the following format of date-time columns to a format sql understands while creating the query:\n",
    "# \"May 19, 2021, 12:08 PM\".\n",
    "\n",
    "TEMPLATE = \"\"\"You are a PostgreSql expert. You are given an input question and a mysql database of 6 tables. You need to create a syntactically correct PostgreSql query to run.\n",
    "You don't need to query for {top_k} results for now. All the input question would be for a client Wow Skin Science, for which the account_id is 71. Use this filter every time you generate a PostgreSql query.\n",
    "\n",
    "Always use following database name:\n",
    "'limechat2'\n",
    "\n",
    "When input question has 'date range keywords', use the SQL phrase syntax example delimited by triple backtick for the date range part in the PostgreSql query.\n",
    "```between (now() - interval '330mins' - interval '1 day') and (now() - interval '330mins')```\n",
    "\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables:\n",
    "...\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\",\"top_k\"], template=TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "# # Replace 'mysql://username:password@hostname:port/database_name' with your database connection details\n",
    "# engine = create_engine('mysql://root:Qwert1234%@localhost:3306/limechat')\n",
    "# # Create a session factory\n",
    "# Session = sessionmaker(bind=engine)\n",
    "\n",
    "# # Now, you can create a session to interact with your MySQL database\n",
    "# db = Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(\"mysql://root:Qwert1234%@localhost:3306/limechat2\")\n",
    "db.table_info\n",
    "# db = SQLDatabase(engine=engine)\n",
    "# db.run('SELECT * from inboxes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n-account:\\n-id: Typically a unique identifier for each record in a database.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, verbose=True,model=\"gpt-3.5-turbo-16k\")\n",
    "# db_chain_prompt = SQLDatabaseChain.from_llm(llm, db, verbose=True, return_direct=True,return_intermediate_steps=True)\n",
    "\n",
    "from loguru import logger\n",
    "from langchain.callbacks import FileCallbackHandler\n",
    "\n",
    "logfile = \"logs/chain_output.log\"\n",
    "logger.add(logfile)\n",
    "\n",
    "handler = FileCallbackHandler(logfile)\n",
    "\n",
    "db_chain_prompt = SQLDatabaseChain.from_llm(llm, db,prompt=PROMPT, top_k=1,verbose=True,use_query_checker=True, return_direct=True,return_intermediate_steps=True,callbacks=[handler])\n",
    "# db_chain_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_chain_prompt) #return_sql=Falseuse_query_checker=Falsequery_checker_prompt=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = db_chain_prompt.llm_chain.prompt.template\n",
    "# print(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "with get_openai_callback() as cb:\n",
    "    result=db_chain_prompt('What is the account id of client \"Traya\"?')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT COUNT(*) AS ticket_count\n",
    "FROM conversations\n",
    "WHERE account_id = 71\n",
    "AND status = 0\n",
    "AND created_at BETWEEN (now() - interval '330mins' - interval '1 week') AND (now() - interval '330mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.run(\"SELECT COUNT(*) AS total_customers_with_email\\nFROM contacts\\nWHERE email IS NOT NULL\") #22196 for all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb\n",
    "# Tokens Used: 4050\n",
    "# ...\n",
    "# Tokens Used: 4050\n",
    "# \tPrompt Tokens: 4046\n",
    "# \tCompletion Tokens: 4\n",
    "# Successful Requests: 1\n",
    "# Total Cost (USD): $0.006077000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['intermediate_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# db = SQLDatabase.from_uri(\"sqlite:///output.db\")\n",
    "# llm = OpenAI(temperature=0, verbose=True)\n",
    "# db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT)\n",
    "# # db_chain = create_sql_query_chain(llm, db)\n",
    "# # db_chain.run(\"How many employees are there?\")\n",
    "# # this example gives us context length limit. Need to manually define the context using above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_chain.llm_chain.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "text = \"This is an example sentence to count tokens.\"\n",
    "token_count = len(encoding.encode(TEMPLATE))\n",
    "print(f\"The text contains {token_count} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_chain.run(\"How many employees are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.first.steps['table_info']=des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db_chain.first.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain.schema.runnable.base import RunnableLambda\n",
    "\n",
    "# Transform DataFrame rows into RunnableLambda objects\n",
    "runnable_lambda_objects = []\n",
    "for index, row in des.iterrows():\n",
    "    runnable_lambda = RunnableLambda(row['table'],row['column'],row['tags'], row['description'], row['foreign_key'])\n",
    "    runnable_lambda_objects.append(runnable_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain.middle[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_chain.middle[0]\n",
    "\n",
    "response = db_chain.invoke({\"question\":\"how many people are there in inbox\"})\n",
    "print(response)\n",
    "# print(\"\"\"PromptTemplate(input_variables=['input', 'table_info', 'top_k'], output_parser=None, partial_variables={}, template='You are a SQLite expert. Given an input question, first create a syntactically correct SQLite query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per SQLite. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use date(\\'now\\') function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables:\\n{table_info}\\n\\nQuestion: {input}', template_format='f-string', validate_template=True)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = PromptTemplate(input_variables=['input', 'table_info', 'table_description'], output_parser=None, partial_variables={}, template=TEMPLATE, template_format='f-string', validate_template=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\"\"SQLDatabaseChain(memory=None, callbacks=None, callback_manager=None, verbose=True, tags=None, metadata=None, llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['input', 'table_info', 'dialect', 'table_description'], output_parser=None, partial_variables={}, template='Given an input query, create a syntactically correct {dialect} query to run.\\nFor your references, you can use the following table for description of each table and description of their respective columns.\\n\\'\\'\\'\\n{table_description}\\n\\'\\'\\'\\n\\nUse the following format:\\nQuestion: \"Question here\"\\nSQLQuery: \"SQL Query to run\"\\n\\nOnly use the following tables:\\n\\n{table_info}.\\n\\nQuestion: {input}', template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=True, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None), output_key='text', output_parser=StrOutputParser(), return_final_only=True, llm_kwargs={}), llm=None, database=<langchain.utilities.sql_database.SQLDatabase object at 0x000001C3F48D9D20>, prompt=None, top_k=5, input_key='query', output_key='result', return_sql=False, return_intermediate_steps=False, return_direct=False, use_query_checker=False, query_checker_prompt=None)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import create_sql_query_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = create_sql_query_chain(ChatOpenAI(temperature=0), db)\n",
    "response = chain.invoke({\"question\":\"how many people are there in inbox\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After building the SQL query based on a user question, we can execute the query:\n",
    "db.run(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "TEMPLATE = \"\"\"Given an input query, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
    "For your references, you can use the following table for description of each table and description of their respective columns.\n",
    "'''\n",
    "{table_description}\n",
    "'''\n",
    "\n",
    "Use the following format:\n",
    "Question: \"Question here\"\n",
    "SQLQuery: \"SQL Query to run\"\n",
    "SQLResult: \"Result of the SQLQuery\"\n",
    "Answer: \"Final answer here\"\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "{table_info}.\n",
    "\n",
    "Some examples of SQL queries that corrsespond to questions are:\n",
    "\n",
    "{few_shot_examples}\n",
    "\n",
    "Question: {input}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    input_variables=[\"input\", \"few_shot_examples\", \"table_info\", \"dialect\",\"table_description\"], template=TEMPLATE\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "we can provide examples using following lines in prompt template\n",
    "\n",
    "- Some examples of SQL queries that corrsespond to questions are:\n",
    "\n",
    "- {few_shot_examples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The performance of the SQLDatabaseChain can be enhanced in several ways:\n",
    "## 1. Adding Sample Rows\n",
    "db = SQLDatabase.from_uri(\n",
    "    \"sqlite:///output.db\",\n",
    "    include_tables=['inboxes'], # we include only one table to save tokens in the prompt :)\n",
    "    sample_rows_in_table_info=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(db.table_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can also use SQL agent in place of It can answer questions based on the databases' schema as well as on the databases' content (like describing a specific table)\n",
    "## It can recover from errors by running a generated query, catching the traceback and regenerating it correctly\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "# from langchain.agents import AgentExecutor\n",
    "from langchain.agents.agent_types import AgentType\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///output.db\")\n",
    "llm = OpenAI(temperature=0, verbose=True)\n",
    "\n",
    "agent_executor = create_sql_agent(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    toolkit=SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0)),\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_executor.run(\n",
    "#     \"List the total sales per country. Which country's customers spent the most?\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"Describe the inboxes table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
